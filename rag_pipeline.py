import os 
import sys
import platform
import pkg_resources
import langchain_community
import langchain_core
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
import openai
from openai import OpenAI
from dotenv import load_dotenv



def version_specs():

    print(f"Python version: {platform.python_version()}")
    print(f"OS: {platform.system()} {platform.release()}")
    print("")
    print(f"langchain_community: {langchain_community.__version__}")
    print(f"langchain_core: {langchain_core.__version__}")
    print(f"openai: {openai.__version__}")
    print("")

    
def api_key_reader():
    
    load_dotenv()
    
    os.environ['OPENAI_API_KEY']       = os.getenv('OPENAI_API_KEY')
    os.environ['LANGCHAIN_API_KEY']    = os.getenv('LANGCHAIN_API_KEY')
    os.environ["LANGCHAIN_API_KEY"]    = os.getenv("LANGCHAIN_API_KEY")   
    os.environ["LANGCHAIN_TRACING_V2"] = "true"    
    

def load_embed_store_doc(file_path, chunk_size=1000, chunk_overlap=20):
    """
    Load a document from the given file path, chunk it, embed the chunks, and store them in a vector store.
    
    Parameters:
    file_path (str): The path to the document file.
    chunk_size (int, optional): The size of each chunk. Default is 1000.
    chunk_overlap (int, optional): The overlap between chunks. Default is 20.
    
    Returns:
    FAISS: The vector store containing the embedded document chunks.
    """
    
    loader = PyPDFLoader(file_path)
    docs   = loader.load()
    
    # Chunk the document using recursive text splitter
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    split_doc     = text_splitter.split_documents(docs)

    # Convert to vector embeddings using OpenAI
    embeddings = OpenAIEmbeddings()
    
    # Store vector embeddings in vectore database (vector store) 
    db = FAISS.from_documents((split_doc), embeddings)
    
    # Verify the vector store contents
    print("Number of documents stored in the vector store:", len(split_doc), "\n")

    return db


def rag_prompt_template(db, query):
    """
    Create a prompt template for RAG using a query.
    
    Parameters:
    db (FAISS): The FAISS vector store containing the document embeddings.
    query (str): The query to retrieve relevant documents.
    
    Returns:
    list: The prepared prompt messages.
    """
    # Retrieve relevant documents
    retriever = db.as_retriever()
    retrieved_docs = retriever.get_relevant_documents(query)
    
    # Define the context
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    
    # Create the messages to send to the API
    messages = [
        {"role": "system", "content": "You are a helpful assistant. Answer the following question based only on the provided context. Think step by step before providing a detailed answer. Explain how you came to the answer."},
        {"role": "user", "content": f"The context is as follows: \n<context>\n{context}\n</context>\nQuestion: {query}"}
    ]
    
    return messages

    
def make_api_request(messages, model="gpt-4-turbo"):
    """
    Make the API request to model API with the given messages.
    
    Parameters:
    messages (list): The messages to send to the model API.
    model (str, optional): The language model to use. Default is "gpt-4-turbo".
    
    Returns:
    str: The response from the model API.
    """    
    # Make the API request
    client = OpenAI()
    
    response = client.chat.completions.create(
        model    = model,
        messages = messages
    )


    # Return the response from the assistant
    # response = print(response.choices[0].message.content.strip())
    # return response
    return response.choices[0].message.content.strip()



    
def rag_execute(file_path, query, chunk_size=1000, chunk_overlap=20, model="gpt-4-turbo"):
    """
    Execute the Retrieval-Augmented Generation (RAG) process.
    
    Parameters:
    file_path (str): The path to the document file.
    query (str): The query to retrieve relevant documents and generate a response.
    chunk_size (int, optional): The size of each chunk for document splitting. Default is 1000.
    chunk_overlap (int, optional): The overlap between chunks for document splitting. Default is 20.
    model (str, optional): The language model to use. Default is "gpt-4-turbo".
    
    Returns:
    str: The response generated by the language model.
    """
    # Print version specifications
    version_specs()
    
    # Read API keys
    api_key_reader()
    
    # Load, embed, and store the document
    db = load_embed_store_doc(file_path, chunk_size, chunk_overlap)
    
    # Prepare the prompt template
    messages = rag_prompt_template(db, query)
    
    # Make the API request
    response = make_api_request(messages, model)
    
    return response

